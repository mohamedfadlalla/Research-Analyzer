{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# preprations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "from prompts import system_prompt_1, system_prompt_2\n",
    "\n",
    "import re\n",
    "import os \n",
    "from IPython.display import Markdown\n",
    "\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_groq import ChatGroq\n",
    "\n",
    "\n",
    "\n",
    "# setup google gemini api keys\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import platform\n",
    "\n",
    "if platform.system() == \"Windows\":\n",
    "    load_dotenv()\n",
    "    GOOGLE_API_KEY = os.getenv(\"GOOGLE_API_KEY\")\n",
    "    GROQ_API_KEY = os.getenv(\"GROQ_API_KEY\")\n",
    "elif platform.system() == \"Linux\":\n",
    "    from google.colab import userdata\n",
    "    GOOGLE_API_KEY = userdata.get('GOOGLE_AI_STUDIO2')\n",
    "    GROQ_API_KEY = userdata.get('GROQ_API_KEY')\n",
    "else:\n",
    "    raise OSError(\"Unsupported operating system\")\n",
    "\n",
    "\n",
    "pro = ChatGoogleGenerativeAI(model=\"gemini-1.5-pro\", google_api_key=GOOGLE_API_KEY, temperature=0.4, convert_system_message_to_human=True)\n",
    "flash = ChatGoogleGenerativeAI(model=\"gemini-1.5-flash\", google_api_key=GOOGLE_API_KEY, temperature=0.3, convert_system_message_to_human=True)\n",
    "\n",
    "llm = ChatGroq(\n",
    "    temperature=0,\n",
    "    # model_name=\"mixtral-8x7b-32768\",\n",
    "    # model_name=\"llama3-70b-8192\",\n",
    "    # model_name=\"llama-3.1-70b-versatile\",\n",
    "    # model_name=\"mixtral-8x7b-32768\",\n",
    "    # model_name=\"llama3-8b-8192\",\n",
    "    model_name=\"llama-3.1-8b-instant\",\n",
    "    # model_name=\"gemma2-9b-it\",\n",
    "    \n",
    "    groq_api_key=GROQ_API_KEY \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yjdblManpnnM",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# PDF Acquisition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "bYq4tu_roBQg"
   },
   "outputs": [],
   "source": [
    "#@title Depandances\n",
    "%%capture\n",
    "!pip3 install -U scidownl\n",
    "# Langchain and groq\n",
    "!pip install langchain\n",
    "!pip install langchain_groq\n",
    "\n",
    "#installing marker\n",
    "!pip install marker-pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 342
    },
    "id": "BdNn168OoOwq",
    "outputId": "db1726c3-2b6e-4636-8101-c7550876da47"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "     <input type=\"file\" id=\"files-1cf9622b-a058-4ff4-9ade-346c2e4185f9\" name=\"files[]\" multiple disabled\n",
       "        style=\"border:none\" />\n",
       "     <output id=\"result-1cf9622b-a058-4ff4-9ade-346c2e4185f9\">\n",
       "      Upload widget is only available when the cell has been executed in the\n",
       "      current browser session. Please rerun this cell to enable.\n",
       "      </output>\n",
       "      <script>// Copyright 2017 Google LLC\n",
       "//\n",
       "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
       "// you may not use this file except in compliance with the License.\n",
       "// You may obtain a copy of the License at\n",
       "//\n",
       "//      http://www.apache.org/licenses/LICENSE-2.0\n",
       "//\n",
       "// Unless required by applicable law or agreed to in writing, software\n",
       "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
       "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
       "// See the License for the specific language governing permissions and\n",
       "// limitations under the License.\n",
       "\n",
       "/**\n",
       " * @fileoverview Helpers for google.colab Python module.\n",
       " */\n",
       "(function(scope) {\n",
       "function span(text, styleAttributes = {}) {\n",
       "  const element = document.createElement('span');\n",
       "  element.textContent = text;\n",
       "  for (const key of Object.keys(styleAttributes)) {\n",
       "    element.style[key] = styleAttributes[key];\n",
       "  }\n",
       "  return element;\n",
       "}\n",
       "\n",
       "// Max number of bytes which will be uploaded at a time.\n",
       "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
       "\n",
       "function _uploadFiles(inputId, outputId) {\n",
       "  const steps = uploadFilesStep(inputId, outputId);\n",
       "  const outputElement = document.getElementById(outputId);\n",
       "  // Cache steps on the outputElement to make it available for the next call\n",
       "  // to uploadFilesContinue from Python.\n",
       "  outputElement.steps = steps;\n",
       "\n",
       "  return _uploadFilesContinue(outputId);\n",
       "}\n",
       "\n",
       "// This is roughly an async generator (not supported in the browser yet),\n",
       "// where there are multiple asynchronous steps and the Python side is going\n",
       "// to poll for completion of each step.\n",
       "// This uses a Promise to block the python side on completion of each step,\n",
       "// then passes the result of the previous step as the input to the next step.\n",
       "function _uploadFilesContinue(outputId) {\n",
       "  const outputElement = document.getElementById(outputId);\n",
       "  const steps = outputElement.steps;\n",
       "\n",
       "  const next = steps.next(outputElement.lastPromiseValue);\n",
       "  return Promise.resolve(next.value.promise).then((value) => {\n",
       "    // Cache the last promise value to make it available to the next\n",
       "    // step of the generator.\n",
       "    outputElement.lastPromiseValue = value;\n",
       "    return next.value.response;\n",
       "  });\n",
       "}\n",
       "\n",
       "/**\n",
       " * Generator function which is called between each async step of the upload\n",
       " * process.\n",
       " * @param {string} inputId Element ID of the input file picker element.\n",
       " * @param {string} outputId Element ID of the output display.\n",
       " * @return {!Iterable<!Object>} Iterable of next steps.\n",
       " */\n",
       "function* uploadFilesStep(inputId, outputId) {\n",
       "  const inputElement = document.getElementById(inputId);\n",
       "  inputElement.disabled = false;\n",
       "\n",
       "  const outputElement = document.getElementById(outputId);\n",
       "  outputElement.innerHTML = '';\n",
       "\n",
       "  const pickedPromise = new Promise((resolve) => {\n",
       "    inputElement.addEventListener('change', (e) => {\n",
       "      resolve(e.target.files);\n",
       "    });\n",
       "  });\n",
       "\n",
       "  const cancel = document.createElement('button');\n",
       "  inputElement.parentElement.appendChild(cancel);\n",
       "  cancel.textContent = 'Cancel upload';\n",
       "  const cancelPromise = new Promise((resolve) => {\n",
       "    cancel.onclick = () => {\n",
       "      resolve(null);\n",
       "    };\n",
       "  });\n",
       "\n",
       "  // Wait for the user to pick the files.\n",
       "  const files = yield {\n",
       "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
       "    response: {\n",
       "      action: 'starting',\n",
       "    }\n",
       "  };\n",
       "\n",
       "  cancel.remove();\n",
       "\n",
       "  // Disable the input element since further picks are not allowed.\n",
       "  inputElement.disabled = true;\n",
       "\n",
       "  if (!files) {\n",
       "    return {\n",
       "      response: {\n",
       "        action: 'complete',\n",
       "      }\n",
       "    };\n",
       "  }\n",
       "\n",
       "  for (const file of files) {\n",
       "    const li = document.createElement('li');\n",
       "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
       "    li.append(span(\n",
       "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
       "        `last modified: ${\n",
       "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
       "                                    'n/a'} - `));\n",
       "    const percent = span('0% done');\n",
       "    li.appendChild(percent);\n",
       "\n",
       "    outputElement.appendChild(li);\n",
       "\n",
       "    const fileDataPromise = new Promise((resolve) => {\n",
       "      const reader = new FileReader();\n",
       "      reader.onload = (e) => {\n",
       "        resolve(e.target.result);\n",
       "      };\n",
       "      reader.readAsArrayBuffer(file);\n",
       "    });\n",
       "    // Wait for the data to be ready.\n",
       "    let fileData = yield {\n",
       "      promise: fileDataPromise,\n",
       "      response: {\n",
       "        action: 'continue',\n",
       "      }\n",
       "    };\n",
       "\n",
       "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
       "    let position = 0;\n",
       "    do {\n",
       "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
       "      const chunk = new Uint8Array(fileData, position, length);\n",
       "      position += length;\n",
       "\n",
       "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
       "      yield {\n",
       "        response: {\n",
       "          action: 'append',\n",
       "          file: file.name,\n",
       "          data: base64,\n",
       "        },\n",
       "      };\n",
       "\n",
       "      let percentDone = fileData.byteLength === 0 ?\n",
       "          100 :\n",
       "          Math.round((position / fileData.byteLength) * 100);\n",
       "      percent.textContent = `${percentDone}% done`;\n",
       "\n",
       "    } while (position < fileData.byteLength);\n",
       "  }\n",
       "\n",
       "  // All done.\n",
       "  yield {\n",
       "    response: {\n",
       "      action: 'complete',\n",
       "    }\n",
       "  };\n",
       "}\n",
       "\n",
       "scope.google = scope.google || {};\n",
       "scope.google.colab = scope.google.colab || {};\n",
       "scope.google.colab._files = {\n",
       "  _uploadFiles,\n",
       "  _uploadFilesContinue,\n",
       "};\n",
       "})(self);\n",
       "</script> "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-fe3794aac9db>\u001b[0m in \u001b[0;36m<cell line: 32>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-4-fe3794aac9db>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0mdownload_paper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mchoice\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'2'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m         \u001b[0mupload_paper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Invalid choice. Please enter 1 or 2.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-fe3794aac9db>\u001b[0m in \u001b[0;36mupload_paper\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mupload_paper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m     \u001b[0muploaded\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfiles\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muploaded\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mendswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'.pdf'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Uploaded file is not a PDF file.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/files.py\u001b[0m in \u001b[0;36mupload\u001b[0;34m()\u001b[0m\n\u001b[1;32m     67\u001b[0m   \"\"\"\n\u001b[1;32m     68\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m   \u001b[0muploaded_files\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_upload_files\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmultiple\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m   \u001b[0;31m# Mapping from original filename to filename as saved locally.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m   \u001b[0mlocal_filenames\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/files.py\u001b[0m in \u001b[0;36m_upload_files\u001b[0;34m(multiple)\u001b[0m\n\u001b[1;32m    162\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    163\u001b[0m   \u001b[0;32mwhile\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'action'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m'complete'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 164\u001b[0;31m     result = _output.eval_js(\n\u001b[0m\u001b[1;32m    165\u001b[0m         'google.colab._files._uploadFilesContinue(\"{output_id}\")'.format(\n\u001b[1;32m    166\u001b[0m             \u001b[0moutput_id\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_id\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/output/_js.py\u001b[0m in \u001b[0;36meval_js\u001b[0;34m(script, ignore_result, timeout_sec)\u001b[0m\n\u001b[1;32m     38\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mignore_result\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0m_message\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_reply_from_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_sec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mread_reply_from_input\u001b[0;34m(message_id, timeout_sec)\u001b[0m\n\u001b[1;32m     94\u001b[0m     \u001b[0mreply\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_read_next_input_message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mreply\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_NOT_READY\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreply\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 96\u001b[0;31m       \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.025\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     97\u001b[0m       \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m     if (\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from scidownl import scihub_download\n",
    "import pandas as pd\n",
    "import os\n",
    "from google.colab import files\n",
    "\n",
    "def download_paper():\n",
    "    paper = input(\"Please enter the DOI of the paper here: \")\n",
    "    paper_type = \"doi\"\n",
    "    out = \"./paper.pdf\"\n",
    "    # Uncomment and configure the proxies if needed\n",
    "    # proxies = {\n",
    "    #     'http': 'socks5://127.0.0.1:7890'\n",
    "    # }\n",
    "    scihub_download(paper, paper_type=paper_type, out=out)#, proxies=proxies)\n",
    "\n",
    "def upload_paper():\n",
    "    uploaded = files.upload()\n",
    "    if not list(uploaded.keys())[0].endswith('.pdf'):\n",
    "        raise ValueError('Uploaded file is not a PDF file.')\n",
    "    os.rename(list(uploaded.keys())[0], 'paper.pdf')\n",
    "    print(\"Paper uploaded successfully as 'paper.pdf'\")\n",
    "\n",
    "def main():\n",
    "    choice = input(\"Choose an option (1 for DOI download, 2 for PDF upload): \")\n",
    "    if choice == '1':\n",
    "        download_paper()\n",
    "    elif choice == '2':\n",
    "        upload_paper()\n",
    "    else:\n",
    "        print(\"Invalid choice. Please enter 1 or 2.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "u871l3MWPQ7c"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I7w_hmCOpvnd",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Markdown Extraction with Marker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TlzWKPpHUHNL"
   },
   "outputs": [],
   "source": [
    "#@title extracting markdown\n",
    "!marker_single /content/paper.pdf /content/output/ --batch_multiplier 2 --max_pages 20 --langs English"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RogGmG7cp8-e"
   },
   "source": [
    "# Extracting the Methodology Section\n",
    "**Objective:** Isolate the methodology section from the segmented markdown for further analysis.\n",
    "- **Action Items:**\n",
    "  - From the parsed sections, specifically extract the text under the \"Methodology\" heading.\n",
    "  - Handle variations in heading titles like \"Methods\" or \"Materials and Methods.\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QGyWoVDrp-A0"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def extract_outline(file_path):\n",
    "    outline = []\n",
    "    with open(file_path, 'r') as file:\n",
    "        for line in file:\n",
    "            header_match = re.match(r'^(#+)\\s+(.*)', line)\n",
    "            if header_match:\n",
    "                level = len(header_match.group(1))\n",
    "                title = header_match.group(2)\n",
    "                outline.append((header_match.group(1), title))\n",
    "    return outline\n",
    "\n",
    "def print_outline(outline):\n",
    "    for hashes, title in outline:\n",
    "        print(hashes + ' ' + title)\n",
    "\n",
    "file_path = 'manuscript.md'\n",
    "outline = extract_outline(file_path)\n",
    "print_outline(outline)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "NN93TF30WvUV"
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'get_markdown' from 'prompts' (C:\\Users\\DELL\\Desktop\\Research Projects\\Research-Analyzer\\prompts.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain_google_genai\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ChatGoogleGenerativeAI\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain_core\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mprompts\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ChatPromptTemplate\n\u001b[1;32m----> 4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mprompts\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m system_prompt_1, system_prompt_2, get_markdown\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mre\u001b[39;00m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m \n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'get_markdown' from 'prompts' (C:\\Users\\DELL\\Desktop\\Research Projects\\Research-Analyzer\\prompts.py)"
     ]
    }
   ],
   "source": [
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "from prompts import system_prompt_1, system_prompt_2, get_markdown\n",
    "\n",
    "import re\n",
    "import os \n",
    "from IPython.display import Markdown\n",
    "\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_groq import ChatGroq\n",
    "\n",
    "\n",
    "\n",
    "# setup google gemini api keys\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "GOOGLE_API_KEY = os.getenv(\"GOOGLE_API_KEY\")\n",
    "\n",
    "gem1 = ChatGoogleGenerativeAI(model=\"gemini-1.5-pro\", google_api_key=GOOGLE_API_KEY, temperature=0.4, convert_system_message_to_human=True)\n",
    "flash = ChatGoogleGenerativeAI(model=\"gemini-1.5-flash\", google_api_key=GOOGLE_API_KEY, temperature=0.3, convert_system_message_to_human=True)\n",
    "\n",
    "\n",
    "GROQ_API_KEY = os.getenv(\"GROQ_API_KEY\")\n",
    "\n",
    "groq_mx = ChatGroq(\n",
    "    temperature=0,\n",
    "    # model_name=\"mixtral-8x7b-32768\",\n",
    "    # model_name=\"llama3-70b-8192\",\n",
    "    # model_name=\"llama-3.1-70b-versatile\",\n",
    "    # model_name=\"mixtral-8x7b-32768\",\n",
    "    # model_name=\"llama3-8b-8192\",\n",
    "    model_name=\"llama-3.1-8b-instant\",\n",
    "    # model_name=\"gemma2-9b-it\",\n",
    "    \n",
    "    groq_api_key=GROQ_API_KEY \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "o0RlXtxQTiiM",
    "outputId": "7e5d35a0-a54f-4a3a-a06c-06d50b3c9e77"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Chapter Two: Materials and Methods\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "system = \"\"\"\n",
    "###instructions###\n",
    "You will receive an outline of a scientific paper in the format shown in the expected input. Extract and present only the highest level heading corresponding to the methods section, with no other output.\n",
    "\n",
    "**Expected Input:**\n",
    "#abstract\n",
    "#introduction\n",
    "#method\n",
    "##method 1\n",
    "#results\n",
    "#conclusion\n",
    "\n",
    "**Expected Output:**\n",
    "#method\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "text = f\"\"\"{outline}\"\"\"\n",
    "prompt = ChatPromptTemplate.from_messages([(\"system\", system), (\"human\", text)])\n",
    "\n",
    "chain = prompt | chat\n",
    "response = chain.invoke({})\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "kkisie-yTgs2"
   },
   "outputs": [],
   "source": [
    "#@title Extracting the method section from the Markdown file\n",
    "import re\n",
    "\n",
    "def extract_section(file_path, section_title):\n",
    "    section_content = []\n",
    "    inside_section = False\n",
    "    section_level = None\n",
    "\n",
    "    with open(file_path, 'r') as file:\n",
    "        for line in file:\n",
    "            header_match = re.match(r'^(#+)\\s+(.*)', line)\n",
    "            if header_match:\n",
    "                level = len(header_match.group(1))\n",
    "                title = header_match.group(2)\n",
    "\n",
    "                if inside_section:\n",
    "                    if level <= section_level:\n",
    "                        break\n",
    "                if header_match.group(1) + ' ' + title == section_title:\n",
    "                    inside_section = True\n",
    "                    section_level = level\n",
    "\n",
    "            if inside_section:\n",
    "                section_content.append(line)\n",
    "\n",
    "    return ''.join(section_content)\n",
    "\n",
    "\n",
    "file_path = 'manuscript.md'\n",
    "section_title = response.content\n",
    "method = extract_section(file_path, section_title)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7v_dig2-h2BD"
   },
   "source": [
    "## Comperhencive Sammary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "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\n"
     ]
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "load_dotenv()\n",
    "GOOGLE_API_KEY = os.getenv(\"GOOGLE_API_KEY\")\n",
    "\n",
    "\n",
    "GROQ_API_KEY = os.getenv(\"GROQ_API_KEY\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_groq import ChatGroq\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from prompts import system_prompt_1, system_prompt_2\n",
    "from input import method_1, method_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "cellView": "form",
    "id": "z0DJ2K0fqCUl"
   },
   "outputs": [],
   "source": [
    "#@title creation of nodes and edges\n",
    "\n",
    "chat = ChatGroq(\n",
    "    temperature=0,\n",
    "    model_name=\"mixtral-8x7b-32768\",\n",
    "    \n",
    "    max_tokens=1024,\n",
    "    groq_api_key=GROQ_API_KEY  # Optional if not set as an environment variable\n",
    ")\n",
    "\n",
    "system = system_prompt_1\n",
    "\n",
    "text = f\"\"\"{method_2}\"\"\"\n",
    "prompt = ChatPromptTemplate.from_messages([(\"system\", system), (\"human\", text)])\n",
    "\n",
    "chain = prompt | chat\n",
    "nande = chain.invoke({})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "cellView": "form",
    "id": "_6LZcAfcGz6S"
   },
   "outputs": [],
   "source": [
    "from prompts import system_prompt_1, system_prompt_2\n",
    "from input import method_1, method_2\n",
    "\n",
    "#@title writing the code for the graphviz\n",
    "\n",
    "chat = ChatGroq(\n",
    "    temperature=0,\n",
    "    model_name=\"gemma2-9b-it\",\n",
    "    max_tokens=1024,\n",
    "    groq_api_key=GROQ_API_KEY  # Optional if not set as an environment variable\n",
    ")\n",
    "\n",
    "system = system_prompt_2\n",
    "\n",
    "text = f\"\"\"{nande.content}\"\"\"\n",
    "prompt = ChatPromptTemplate.from_messages([(\"system\", system), (\"human\", text)])\n",
    "\n",
    "chain = prompt | chat\n",
    "llm_response = chain.invoke({})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "cellView": "form",
    "id": "uOKa35CqIEJt"
   },
   "outputs": [],
   "source": [
    "#@title extract the code form the response and run it\n",
    "import re\n",
    "\n",
    "def extract_code(llm_response):\n",
    "    # Define the pattern to match the Python code block\n",
    "    pattern = re.compile(r'```python(.*?)```', re.DOTALL)\n",
    "\n",
    "    # Search for the code block\n",
    "    match = pattern.search(llm_response)\n",
    "\n",
    "    if match:\n",
    "        # Extract the code\n",
    "        code = match.group(1).strip()\n",
    "        return code\n",
    "    else:\n",
    "        return \"No Python code found in the response.\"\n",
    "\n",
    "# Example LLM response\n",
    "code = extract_code(llm_response.content)\n",
    "exec(code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "### instructions ###\n",
      "Your job is to write Python code that:\n",
      "1. Reads the provided list of nodes and edges.\n",
      "2. Utilizes Graphviz to create a directed graph.\n",
      "3. Adds nodes to the graph with appropriate labels.\n",
      "4. Adds edges between the nodes with descriptions of their relationships.\n",
      "5. Sets aesthetic properties such as layout direction, size, shape, or other visual attributes.\n",
      "7. Very important to output the results to high quality png image name \"graph.png\". \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(system_prompt_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comperhencive sammary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatGroq(\n",
    "    temperature=0,\n",
    "    # model_name=\"mixtral-8x7b-32768\",\n",
    "    # model_name=\"llama3-70b-8192\",\n",
    "    # model_name=\"llama-3.1-70b-versatile\",\n",
    "    # model_name=\"mixtral-8x7b-32768\",\n",
    "    model_name=\"llama3-8b-8192\",\n",
    "    # model_name=\"llama-3.1-8b-instant\",\n",
    "    # model_name=\"gemma2-9b-it\",\n",
    "    \n",
    "    groq_api_key=GROQ_API_KEY \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatGroq(\n",
    "    temperature=0,\n",
    "    # model_name=\"mixtral-8x7b-32768\",\n",
    "    # model_name=\"llama3-70b-8192\",\n",
    "    # model_name=\"llama-3.1-70b-versatile\",\n",
    "    # model_name=\"mixtral-8x7b-32768\",\n",
    "    # model_name=\"llama3-8b-8192\",\n",
    "    model_name=\"llama-3.1-8b-instant\",\n",
    "    # model_name=\"gemma2-9b-it\",\n",
    "    \n",
    "    groq_api_key=GROQ_API_KEY \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "\n",
       "##  Background and Objectives   \n",
       "The COVID-19 pandemic, caused by the SARS-CoV-2 virus, has become a global health crisis. The study aims to investigate the potential of repurposing FDA-approved drugs against SARS-CoV-2 virus using molecular docking and pharmacophore modeling.\n",
       "##  Methods                     \n",
       "**Methodology Summary:**\n",
       "\n",
       "* Design: Systematic search of scientific studies on Google Scholar\n",
       "* Sample: All studies published by the end of 2020/12/30\n",
       "* Tools/Techniques: \n",
       "  - Keywords: (COVID19 OR SARS-CoV-2) AND (Drug repurposing OR Drug repositioning OR Drug re-profling OR Drug rediscovery) AND (Docking AND Molecular dynamic)\n",
       "  - Database: Google Scholar\n",
       "##  Main Findings               \n",
       "* 1,444 studies on repurposing drugs for COVID-19 were collected from Google Scholar by the end of 2020/12/30.\n",
       "##  Analysis and Interpretation \n",
       "The authors used a systematic search strategy in Google Scholar to collect studies on drug repurposing for COVID-19, using a combination of keywords. The search was limited to studies published by the end of 2020/12/30.\n",
       "\n",
       "The authors did not mention any specific statistical methods used in the analysis. However, the results of the search strategy are likely to be presented in the form of a list of studies, which may be analyzed using descriptive statistics (e.g., frequency, percentage) to summarize the findings.\n",
       "\n",
       "The implications of the results are not explicitly stated, but the authors likely aim to provide an overview of the existing literature on drug repurposing for COVID-19, which can inform future research and clinical decisions.\n",
       "##  Contributions and Novelty   \n",
       "The paper contributes to the field of COVID-19 research by providing a comprehensive search strategy for identifying studies on repurposing drugs for COVID-19. The novel approach proposed is the use of a combination of keywords in the Google Scholar database to efficiently collect relevant studies.\n",
       "##  Limitations                 \n",
       "The study may be limited by the reliance on Google Scholar, which may not be comprehensive in capturing all relevant studies, particularly those published in non-English languages or in non-indexed journals. Additionally, the search strategy may be biased towards studies that used docking and molecular dynamics, potentially overlooking other repurposing methods.\n",
       "##  Conclusions and Recommendations \n",
       "The researchers conclude that antiviral drugs, particularly those targeting the main protease, show promise in the fight against COVID-19. Tetracyclines and antivirals, originally protease inhibitors, are recommended for further investigation. The models developed in this study can be used for virtual screening to aid in finding effective therapeutic agents against COVID-19.\n",
       "##  Key References and Citations \n",
       "Number of citaiton: 108"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Manuscript to pandas\n",
    "import summary\n",
    "import importlib\n",
    "importlib.reload(summary)\n",
    "\n",
    "file_path = 'Examples/systematic_review/systematic_review.md'\n",
    "results = summary.summarize(file_path, llm)\n",
    "Markdown(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('file.txt', 'w') as f:\n",
    "    f.write(results)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "IlckXLp5p-7c"
   ],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
